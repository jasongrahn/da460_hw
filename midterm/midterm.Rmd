---
title: "midterm"
author: "jason grahn"
date: "10/26/2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(moderndive)
library(psych)
```

#Problem 1:
##N. 1-1 
Download smoking.csv (or smoking.txt) and read corresponding data into R. Example command in R: `MyData <- read.csv(file="path/TheDataIWantToReadIn.csv",header=TRUE, sep=",")`Make sure to include the code/command.

```{r loading smoking data}
library(readr)
smoking <- read_csv("smoking.csv")
#View(smoking)
```

##N. 1-2 
How many observations are there in this data set? 
How many variables, and what are they? 

```{r}
#how many observations and variables are shown with glimpse()
glimpse(smoking)
```

The `glimpse()` function shows us both the number of variables and rows. The number of rows in the data is `r nrow(smoking)`, and the number of variables is 12. They are:
    
    * gender
    * age
    * maritalStatus
    * highestQualification
    * nationality
    * ethnicity
    * grossIncome
    * region
    * smoke
    * amtWeekends
    * amtWeekdays
    * type

```{r}
#What is the 300th observation of nationality?
smoking$nationality[[300]]
```

##N. 1-3 
Create a numerical summary for age and compute the interquartile range. 
```{r}
summary(smoking$age)

age.iqr <- describe(smoking$age, IQR = TRUE) %>% 
  select(IQR)
```

The numerical summary for `age` is shown above. The IQR is `r age.iqr$IQR`.

Compute the relative frequency distribution for gender. 
```{r}
table(smoking$gender)/nrow(smoking)
```

How many males are in the sample? Include both the code/command and the output/graph.
```{r}
male.count <- nrow(
  smoking %>% 
    filter(gender == "Male")
)
```

There are `r male.count` males in the sample. 

##N. 1-4
Using numerical summaries and a side-by-side box plot, determine if male smokers are as old as female smokers. Include both the code/command and the output/graph.

```{r setup smoking filtered data}
s.g <- smoking %>% 
  filter(smoke == "Yes") %>% 
  select(age, gender)

#group the male data together
s.g.m <- s.g %>% filter(gender == "Male")

#group the female data together
s.g.f <- s.g %>% filter(gender == "Female")
```

```{r summary of make smoker ages}
#age summary for male smokers
summary(s.g.m$age)
```

```{r summary of female smoker ages}
#age summary for female smokers
summary(s.g.f$age)
```

```{r side by side boxplot of smoker ages}
s.g %>% 
  ggplot(aes(x = gender, y = age)) + 
  geom_boxplot() +
  geom_jitter(alpha = 0.1, width = 0.05) +
  theme_minimal()
```

##N. 1-5
Create a bar chart or frequency table for maritalStatus, what is the proportion for Divorced, Single, Married, and Widowed, respectively? What can you interpret from these numbers? Include both the code/command and the output/graph.

```{r status frequency table}
table(smoking$maritalStatus)/nrow(smoking) 
```

We can interpret from these numbers that most people in the smokers data frame are married. Single is next, then widowed. 

#Problem 2
#N. 2-1
Suppose we’re flipping an unfair coin that we know only lands heads 30% of the time. Please simulate this flip 10 times, what is the proportion of heads? 
```{r}
outcomes.coin <- c("heads", "tails")

#build a dataset of an UNFAIR coin toss
sim_unfair_coin <- sample(outcomes.coin, size = 10, replace = TRUE, prob = c(0.3, 0.7))

unfair.tidy <- tibble(sim_unfair_coin)

prop.heads <- unfair.tidy %>% 
  group_by(sim_unfair_coin) %>% 
  summarise(coin_count = n()) %>%
  mutate(freq = coin_count / nrow(unfair.tidy)) %>% 
  filter(sim_unfair_coin == "heads") %>% 
  select(freq)
```

The proportion of heads is `r prop.heads`.

If you simulate this flip 100 times, what is the proportion of heads now? Include both the code/command and the output/graph.

```{r}
sim_unfair_coin.large <- sample(outcomes.coin, size = 100, replace = TRUE, prob = c(0.3, 0.7))

unfair.tidy.large <- tibble(sim_unfair_coin)

prop.heads.large <- unfair.tidy.large %>% 
  group_by(sim_unfair_coin) %>% 
  summarise(coin_count = n()) %>%
  mutate(freq = coin_count / nrow(unfair.tidy)) %>% 
  filter(sim_unfair_coin == "heads") %>% 
  select(freq)
```

The proportion of heads is `r prop.heads.large`.

##N.2-2
Suppose we’re flipping an unfair dice and the corresponding probability of landing 1, 2,
3, 4, 5, and 6 is 0.05, 0.1, 0.15, 0.2, 0.3, and 0.2, respectively. If you simulate this flip 10
times, what is the proportion of land on side 5? 
```{r}
outcomes.dice <- c(1, 2, 3, 4, 5, 6)

#build a dataset of an UNFAIR coin toss
sim.unfair.dice <- sample(outcomes.dice, size = 10, replace = TRUE, prob = c(0.05, 0.1, 0.15, 0.2, 0.3,  0.2))

unfair.dice.tidy <- tibble(sim.unfair.dice)

prop.5 <- unfair.dice.tidy %>% 
  group_by(sim.unfair.dice) %>% 
  summarise(dice.count = n()) %>%
  mutate(freq = dice.count / nrow(unfair.dice.tidy)) %>% 
  filter(sim.unfair.dice == 5) %>% 
  select(freq)
```

The proportion of landing on side `5` is `r prop.5`.

Simulate this flip 100 times, what is the proportion of side 5 now? Include both the code/command and the output/graph.

```{r}
#build a dataset of an UNFAIR coin toss
sim.unfair.dice.large <- sample(outcomes.dice, size = 100, replace = TRUE, prob = c(0.05, 0.1, 0.15, 0.2, 0.3, 0.2))

unfair.dice.tidy.large <- tibble(sim.unfair.dice.large)

prop.5.large <- unfair.dice.tidy.large %>% 
  group_by(sim.unfair.dice.large) %>% 
  summarise(dice.count = n()) %>%
  mutate(freq = dice.count / nrow(unfair.dice.tidy.large)) %>% 
  filter(sim.unfair.dice.large == 5) %>% 
  select(freq)
```

The proportion of landing on side `5` is `r prop.5.large`.

##N. 2-3
Compare the proportions in each questions above, what conclusion can you draw? Does the number of simulations affect the proportions? If so, how? Please explain in details. 

Running the simulation only 10 times, we have a situation where we might not land on a give value _at all_. However, with running more simulations, we get ever closer to the probabilities provided for the cheater dice. Even with a cheater dice, this is due to central limit theorem. With the cheater dice, we've set the conditions, from there, the dice rolls are independent in that rolling once has no conditional bearing on what happens afterward, AND we have a sufficiently high number of dice rolls... though there is at least some diminished independece as we HAVE set the conditional parameters. 

#Problem 3:
##N. 3-1

Download countyComplete.csv (or countyComplete.txt) and read corresponding data into R. Example command in R: `MyData <- read.csv(file="path/TheDataIWantToReadIn.csv", header=TRUE, sep=",")`. Include the
code/command.

```{r importing}
countyComplete <- read_csv("countyComplete.csv")
#glimpse(countyComplete)
```

##N. 3-2
Make a histogram of pop2010, how can you descript its distribution, bellshaped or normal? Is it right skewed or left sewed? Include both the code/command and the output/graph.
```{r}
countyComplete %>% 
  select(pop2000, pop2010) %>% 
  psych::describe() %>% 
  select(skew) 

countyComplete %>% 
  select(pop2010) %>% 
  ggplot() +
  geom_histogram(aes(x = pop2010), bins = 100) + 
  theme_minimal()
```

The distribution of `pop2010` definitely has a strong right skew. Based on this, I would NOT treat the population data as "normal". When we evaluate the `skew` of pop2010, we also see it has a value of 14.34 (down from the skewness of pop2000). This is a clear indicator toward the right skewness of the data. 

##N. 3-3
Create a new subset named Washington which contains only the observations of Washington, and then make a histogram of pop2010, how can you describe its distribution, bell-shaped or normal? Is it right skewed or left sewed? Compare this with question 1. Include both the code/command and the output/graph. 
```{r}
wa.subset <- countyComplete %>% 
  filter(state == "Washington")

wa.subset %>% 
  select(pop2000, pop2010) %>% 
  psych::describe() %>% 
  select(skew) 

wa.subset %>% 
  ggplot() +
  geom_histogram(aes(x = pop2010), bins = 100) + 
  theme_minimal()
```

The 2010 data from `Washington` is much less skewed than the entire `countyComplete` data, with a skewness of 3.71. We still see a right tail, but it is much less pronounced.

##N. 3-4
Based on the subset Washington, make a normal probability plot of pop2010. 
```{r qqplot for washington subset data}
wa.subset %>% 
  ggplot(aes(sample = pop2010)) + 
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Quantile-Quantile plot for the Washington sample data")
```

Do all of the points fall on the line? 

Most defininitely do many points _not_ fall on the line. 

How does this plot compare to the probability plot of the original data? Include both the code/command and the output/graph.
```{r qqplot for entirety of dataset}
countyComplete %>% 
  #filter(state != "Washington") %>% 
  ggplot(aes(sample = pop2010)) +
  stat_qq() +
  #scale_y_log10() + if I were going to model this, it'd have to be in log scale. 
  stat_qq_line(color = "red") +
  labs(title = "Quantile-Quantile plot for the sample data")
```

When we evaluate the qqplot for Washington against the entire population, the charts show similar behaviors. Each provides indications of long right tails with their rapid appearance of growth on the far right. The numbers are too large to see, but I'm willing to bet the left side of the entire population is slightly curved upward, similar to Washington. 

##N. 3-5
Suppose the variable pop2010 has a normal distribution, what is the probability that pop2010 is greater than 102,410? 
```{r}
county.pop <- countyComplete$pop2010

pnorm(q = 102410, mean = mean(county.pop), sd = sd(county.pop))
```
Assuming normalicy, the probability `pop2010` is greater than 102,410 is `r pnorm(q = 102410, mean = mean(county.pop), sd = sd(county.pop))`. 

What is the probability that pop2010 is between 190,000 and 1,000,000? Include both the code/command and the output/graph.
```{r}
(oneninety.thou <- pnorm(q = 190000, mean = mean(county.pop), sd = sd(county.pop)))

(one.mil <- pnorm(q = 1000000, mean = mean(county.pop), sd = sd(county.pop)))

one.mil - oneninety.thou
```

The probability that pop2010 is between 190,000 and 1,000,000 is `r one.mil - oneninety.thou`. 

#Problem 4
##N. 4-1
Collect a simple random sample of size 50 from pop2010 in data set countyComplete.csv (or countyComplete.txt). Describe the distribution of this sample. How does it compare to the distribution of the population? Using this sample, what is your best point estimate of the population mean? Include both the code/command and the output/graph.
```{r}
sample.county <- countyComplete %>% 
  select(pop2010) %>% 
  rep_sample_n(size = 50, reps = 1) 

sample.county.histogram <- sample.county %>% 
  ggplot() +
  geom_histogram(aes(pop2010)) +
  labs(title = "histogram for sample size 50")

sample.stats <- sample.county %>% 
  describe()

sample.county.histogram

sample.stats

population.stats <- countyComplete %>% 
  select(state, pop2010) %>% 
  describe()

population.histogram <- countyComplete %>% 
  ggplot() + 
  geom_histogram(aes(x = pop2010))

population.histogram
population.stats
```

The distribution of the sample has a mean of `125,291` with a median `22,695` so already we can tell this is a right skewed sample. When we look at the variable `skew`, we see it has a value `6.04` which reinforces the right bias of the variable. The spread is over 1 million. The sample would indicate the population mean is `108741`.

##N. 4-2
Now, collect a simple random sample of size 300 from pop2010 in data set countyComplete.csv (or countyComplete.txt). Describe the distribution of this sample. How does it compare to the distribution of the population? Using this sample, what is your best point estimate of the population mean? Compare your point estimates from the question above (question 1), what conclusion can you draw? Include both the code/command and the output/graph.
```{r}
sample.county.large <- countyComplete %>% 
  select(pop2010) %>% 
  rep_sample_n(size = 300, reps = 1) 

sample.county.large.histogram <- sample.county.large %>% 
  ggplot() +
  geom_histogram(aes(pop2010)) +
  labs(title = "histogram for sample size 300") 
  #this histogram looks much more normal with a log-scale X. 
  #scale_x_log10() 

sample.county.large.histogram

sample.stats.large <- sample.county.large %>% 
  describe()

sample.stats.large
```

The question of distribution of this dataset is starting to feel quite redundant as the data isn't normal to begin with.. The distribution of the sample has a mean of `125,291` with a median `22,695` so already we can tell this is a right skewed sample. When we look at the variable `skew`, we see it has a value `6.04` which reinforces the right bias of the variable. The spread is over 1 million. The sample would indicate the population mean is `99847`, where the true mean is `98233`. The larger sample better approximates the true mean.

##N. 4-3
3. Create and compare histograms of sample size 50 and 300, which one is closer to symmetry? Why or why not? Include both the code/command and the output/graph.
```{r}
#We drew the histograms already, so we only need to call them back again.
sample.county.histogram
sample.county.large.histogram
```

What we remember from the population.histogram is that the bin for the lowest value was somewhere in the thousands. Both sample sizes have relatively similar shaped distributions to each other and to the population with extremely long right tails and zero symmetry. Neither of these are symmetrical samples and neither will _ever_ be symmetrical samples unless we utilize a log-scale for the X axis. 

