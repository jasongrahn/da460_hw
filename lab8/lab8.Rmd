---
title: "lab8"
author: "jason grahn"
date: "11/24/2018"
output: word_document
---

#The R Part
```{r setup, include=FALSE}
#grab packages
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(modelr)
library(moderndive)
library(infer)
library(corrgram)

#grab data
download.file("http://www.openintro.org/stat/data/evals.RData", 
              destfile = "evals.RData")
load("evals.RData")
```

##1
###Is this an observational study or an experiment? The original research question posed in the paper is whether beauty leads directly to the differences in course evaluations. Given the study design, is it possible to answer this question as it is phrased? If not, rephrase the question.

This is an observational study. The primary indicator is that there are no control or experimental groupings. With an observation study we may not be able to provide causation, but we can build a string inference based on correlation. We *can* ask if an instructors beauty has a positive or negative correlation on student course evaluation.

##2
###Describe the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not?
```{r}
evals %>% 
  ggplot() + 
  theme_light() + 
  geom_histogram(aes(x = score))

summary(evals$score)
```

The distribution of `score` is unimodal with a strong right skew. From this graph, we could infer that students tend to give higher scores more often than lower scores. We would _expect_ a normal distribution, but with a `mean` of 4.175 and a `median` of 4.3, . Perhaps this is related to some biased questions within the evaluation? 

##3
###Excluding score, select two other variables and describe their relationship using an appropriate visualization (scatterplot, side-by-side boxplots, or mosaic plot).
```{r}
evals_plot <- evals %>% 
  ggplot() + 
  theme_light()

evals_plot +
  geom_boxplot(aes(x = pic_color, y = bty_avg)) 

evals_plot + 
  geom_histogram(aes(bty_avg)) + 
  facet_grid(rows = vars(pic_color))
```

The boxplot showcases that, while the spread of `bty_avg` for a color picture is larger with higher uppper scores, a black & white picture has a higher assocated `bty_avg`. There seems to be a relationship here. 

The side by side histograms reflect the same information that the boxplots related, but _also_ show that color pictures simply receive more scores more often.

##3
###Replot the scatterplot, but this time use the function jitter() on the \(y\)- or the \(x\)-coordinate. (Use ?jitter to learn more.) What was misleading about the initial scatterplot?
```{r}
#the original plot
plot(evals$score ~ evals$bty_avg)

#ggplot is easier to modify than plot() when it comes to additional commands.
evals_plot + 
  geom_jitter(aes(bty_avg, score), alpha = 0.5)
```

The original plot was misleading because it didn't showcase all available points. Where the original simply overlays points on top of each other, adding `jitter` to a scatterplot we can differentuate these overlapping points. Another way of doing this would be heavy use of `alpha` which shows positions with more dense point population by making them darker. 

##4
###Letâ€™s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor score by average beauty rating and add the line to your plot using abline(m_bty). Write out the equation for the linear model and interpret the slope. Is average beauty score a statistically significant predictor? Does it appear to be a practically significant predictor?
```{r}
#according to the book
m_bty <- lm(score ~ bty_avg, data = evals)

plot(jitter(evals$score) ~ jitter(evals$bty_avg))
abline(m_bty)
summary(m_bty)

#and the more usable version using geom_smooth for the LM line of that 
#relationship: 
evals %>% 
  ggplot(aes(x = bty_avg, y = score)) +
  theme_light() +
  geom_jitter() +
  geom_smooth(method = lm)

cor(evals$score, evals$bty_avg)
get_regression_table(m_bty)
```

The linear model for this relationship is 

    score = 0.06664*bty_avg + 3.88034

The interpretation for the slope *0.06664* is that for every positive increment change in `bty_avg`, we reflect a *0.06664* positive change in evaluation score.

Running anova(m_bty) reflects that using bty_avg is a statisically significant predictor with a p-value of approximately zero. However, the slope is so low that this does not appear to be a _practically_ significant predictor.

```{r eval=FALSE, include=FALSE}
library(corrgram)
corrgram(evals %>% 
           select(score,age,cls_perc_eval,cls_did_eval,cls_students,bty_avg), 
         order=TRUE, 
         lower.panel=panel.ellipse,
         upper.panel=panel.pts, 
         text.panel=panel.txt,
         diag.panel=panel.minmax, 
         main="MLB11 Data in PC2/PC1 Order")
```

##5
###Use residual plots to evaluate whether the conditions of least squares regression are reasonable. Provide plots and comments for each one (see the Simple Regression Lab for a reminder of how to make these).
```{r least squares regression plot}
#borrowed this code from https://drsimonj.svbtle.com/visualising-residuals to build a linear regression plot
# Steps 1 and 2 build the model and make it a data frame
d <- lm(score ~ bty_avg, data = evals) %>% 
       broom::augment()

# Steps 3 and 4 build the plot
ggplot(d, aes(x = bty_avg, y = score)) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  geom_segment(aes(xend = bty_avg, yend = .fitted), alpha = .2) +  
  # Note `.fitted`
  geom_point(aes(alpha = abs(.resid))) +  # Note `.resid`
  guides(alpha = FALSE) +
  geom_point(aes(y = .fitted), shape = 1) +  # Note `.fitted`
  theme_bw()
```

This is merely a visualization for the gut-check. My initial response is that thre are a huge collection of potential outliers and we'll end up rejecting this model.

```{r}
bty_regression_points <- get_regression_points(
  lm(score ~ bty_avg, data = evals)
  )

bty_regression_points %>% 
  ggplot() + 
  theme_light() +
  geom_histogram(aes(residual), fill = "lightgrey", color = "darkgrey")

#describe(bty_regression_points)
```

The histogram of residuals shows left skewness, which is reaffirmed with a skew factor of `-0.71`. This isn't terrible, but it also isn't great.

```{r qqplot}
ggplot(bty_regression_points, aes(sample = residual)) +
  stat_qq() +
  stat_qq_line(color = "red")
```

Though points generally hug the line, this QQ plot drops off on the upper end by quite a bit. 

```{r histogram}
bty_regression_points %>% 
  ggplot() +
  geom_jitter(aes(bty_avg, residual), alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red")
```

The residual plot for bty shows weight under the zero residual line, though they DO appear generally constant, without any specific sort of pattern.


Conditions for the least squares line are Linearity, Nearly Normal residuals, and Constant Variability.

    * Linearity: The data show generally positive linearity.
    * Nearly Normal residuals: The Histogram of residuals show a left skewed distribution. The residuals normal probability (qq) plot show that the points fall off the line at the bottom by a bit, and most definitely fall from the line at the upper end.
    * Constant Variability: From the residual plot, we can observe that there seems to have constant variability.

We have to assume independence given we are not provided inforamation regarding the sampling methods. 

##7
###P-values and parameter estimates should only be trusted if the conditions for the regression are reasonable. Verify that the conditions for this model are reasonable using diagnostic plots.

```{r}
m_bty_gen <- lm(score ~ bty_avg + gender, data = evals)

m_bty_regression_points <- get_regression_points(m_bty_gen)

m_bty_regression_points %>% 
  ggplot() + 
  theme_light() +
  geom_histogram(aes(residual), fill = "lightgrey", color = "darkgrey") +
  labs(title = "Histogram of residuals")

m_bty_regression_points %>% 
  ggplot(aes(sample = residual)) +
  theme_light() +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "QQ of residuals")

m_bty_regression_points %>% 
  ggplot() +
  geom_point(aes(bty_avg, residual), alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Residual plot")

m_bty_regression_points %>% 
  ggplot() +
  theme_light() + 
  geom_boxplot(aes(gender, residual)) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "gender vs residuals boxplot")

m_bty_regression_points %>% 
  ggplot() +
  theme_light() + 
  geom_boxplot(aes(gender, score)) +
  #geom_hline(yintercept = 0, color = "red") +
  labs(title = "gender vs score boxplot")
```

There's at least 16 different plots we could make of these data, so I hope the above suffice. What we learn from looking at them can be simplified into a few succinct statements: 

    * Residuals appear to be nearly normal.
    * Variability within bty_avg and residuals is still nearly constant.
    * Each variable seems to be related to the outcome with some degree of linearity.

That said, some potential faults: 

    1. The histogram of residuals still suggest a left skew.
    2. The residuals still drop off the line on the upper end of the normal probability(qq) plot.

##8
###Is bty_avg still a significant predictor of score? Has the addition of gender to the model changed the parameter estimate for bty_avg?
```{r}
summary(m_bty_gen)
```

Yes, `bty_avg` is still a _statistically_ valid predictor for score. Adding the gendermale parameter to the model has increasedthe parameter estimate for `bty_avg` from 0.06664 to 0.07416. It still isn't very practial.  

The Multiple R-Squared is rather poor at 0.059. While we might have proper statistical significance, I wouldn't use this model. 

##9
###What is the equation of the line corresponding to males? (Hint: For males, the parameter estimate is multiplied by 1.) For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?
```{r}
multiLines(m_bty_gen)
get_regression_table(m_bty_gen)
```

The equation for the line corresponding to males is 

    Score = 3.747 + 0.172 + 0.074(avg_bty). 
    
The equation for females is 

    Score = 3.747 + 0.074(avg_bty)

The effect here is that for any given point on the line, a male professor will have a score approximately 0.172 points higher than a given female.

##10
###Create a new model called m_bty_rank with gender removed and rank added in. How does R appear to handle categorical variables that have more than two levels? Note that the rank variable has three levels:  teaching, tenure track, tenured.
```{r}
m_bty_rank <- evals %>% 
  lm(score ~ bty_avg + rank, data = .)
summary(m_bty_rank)
multiLines(m_bty_rank)
```

For a categorical variable with greater than two levels, R creates additional variables for every level greater than 1. Here, we've created `ranktenure track` and `ranktenured`. As a result, we must interpret the coefficients a little differently. With "teaching" rank as the base 0, knowing what rank-level a given professor has (apparently) negative consequences to their output score.

##11
###Which variable would you expect to have the highest p-value in this model? Why? Hint: Think about which variable would you expect to not have any association with the professor score.

I think `pic_color` has the least association with `score` because the color of their picture would have nothing to do with how well (or not) they teach. I choose that.

##12
###Check your suspicions from the previous exercise. Include the model output in your response.
```{r kitchen sink model}
kitchen.sink <- lm(score ~ ., data = evals)

get_regression_table(kitchen.sink) %>% 
  arrange(desc(p_value)) %>% 
  select(term, p_value) %>% 
  mutate(is_max = if_else(p_value == (max(p_value)),
                          TRUE,
                          FALSE)) %>% 
  filter(is_max == TRUE)

summary(kitchen.sink)
```

I was wrong; `pic_color` is not the highest (worst) p-value for the model. The highest p-value comes from `cls_profssingle` with a p-value of *0.778*. This originally comes from the `cls_prof` variable from the data. 

##13
###Interpret the coefficient associated with the ethnicity variable.
```{r}
get_regression_table(kitchen.sink) %>% 
  filter(term == "ethnicitynot minority")
```

The `ethnicity` variable was changed to `ethnicitynot minority` because it is a categorical variable and selected _minority_ as the base. If other components of the model are equal, non-minority professors will score approximately 0.123 higher than a minority professor. Note that this also has a fairly high p_value and may be removed from the model with further analysis.

##14
###Drop the variable with the highest p-value and re-fit the model. Did the coefficients and significance of the other explanatory variables change? (One of the things that makes multiple regression interesting is that coefficient estimates depend on the other variables that are included in the model.) If not, what does this say about whether or not the dropped variable was collinear with the other explanatory variables?
```{r}
#dropping cls_prof from the model
no.cls_profssingle <- lm(score ~ rank + ethnicity + gender + language + age + cls_perc_eval + cls_did_eval + cls_students + cls_level + cls_credits + bty_f1lower + bty_f1upper + bty_f2upper + bty_m1lower + bty_m1upper + bty_m2upper + bty_avg +  pic_outfit + pic_color, data = evals)

summary(no.cls_profssingle)
```

Yes, dropping cls_prof from the model changed the p-value and estimates of the variables, some greater than others. I also notice that our Adjusted R-sq has increased. 

##15
###Using backward-selection and p-value as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.

The final model, using backward steps to p-hack our way to significance under approx 0.05 is: 

```{r start your p hacking engines}
#get_regression_table(lm(score ~ ethnicity + gender + language + cls_perc_eval + cls_credits + bty_f1upper + pic_color,data = evals))
#arrange(desc(p_value))
p.hack.backstep <- lm(score ~ ethnicity + gender + language + cls_perc_eval + cls_credits + bty_f1upper + pic_color,data = evals)
summary(p.hack.backstep)
```

Alternatively, we can use `fastbw()` from the `rms` package to automate this:

```{r or automate our p hacking engines}
library(rms)
kitchen.sink.2 <- ols(score ~ ., data=evals)

rms.phack.ols <- fastbw(fit=kitchen.sink.2, 
                        rule="p", 
                        sls=0.5)
rms.phack.ols
```

This results in the same final model with a lot less coding, resulting in less errors.

```{r output of p hacking}
rms.phack.model <- lm(score ~ ethnicity + language + gender + cls_perc_eval + cls_credits + bty_f1upper + pic_color, data = evals)
summary(rms.phack.model)
```

But we all know that P-hacking our way to significance is bad practice and can result in over-fitting. Alternatively, we use `step()` from the stats package to run: _step(kitchen.sink, direction = "backward")_ which gives us a model with the lowest AIC of -642.28 and a higher Adjusted R-squared of 0.1821.

    Step:  AIC=-642.28
    score ~ rank + ethnicity + gender + language + age + cls_perc_eval + 
        cls_credits + bty_f1lower + bty_f1upper + bty_f2upper + bty_avg + 
        pic_outfit + pic_color


                    Df Sum of Sq    RSS     AIC
    <none>                       108.39 -642.28
    - pic_outfit     1    0.5065 108.89 -642.12
    - bty_f1lower    1    0.5212 108.91 -642.06
    - rank           2    1.2018 109.59 -641.18
    - ethnicity      1    1.0217 109.41 -639.94
    - language       1    1.0886 109.48 -639.65
    - bty_avg        1    1.4304 109.82 -638.21
    - bty_f2upper    1    1.7306 110.12 -636.95
    - age            1    1.9987 110.39 -635.82
    - pic_color      1    2.2613 110.65 -634.72
    - bty_f1upper    1    2.2891 110.68 -634.60
    - cls_perc_eval  1    2.3203 110.71 -634.47
    - cls_credits    1    4.9069 113.30 -623.78
    - gender         1    5.7484 114.14 -620.35

    Call:
    lm(formula = score ~ rank + ethnicity + gender + language + age + 
        cls_perc_eval + cls_credits + bty_f1lower + bty_f1upper + 
        bty_f2upper + bty_avg + pic_outfit + pic_color, data = evals)

```{r but AIG modelling is better}
summary(
  lm(formula = score ~ rank + ethnicity + gender + language + age + 
        cls_perc_eval + cls_credits + bty_f1lower + bty_f1upper + 
        bty_f2upper + bty_avg + pic_outfit + pic_color, data = evals)
)
```

But fine, since we're using p-hacking, the rest of the code will be based on `lm(score ~ ethnicity + language + cls_perc_eval + cls_credits + bty_f1upper + pic_color)`. 

The model is: 3.385 + (ethnicitynot minority) x0.179 + (languagenon-english) x -0.22 + (cls_perc_eval)	x 0.005 + (cls_creditsone credit)	x 0.498 + (bty_f1upper) x 0.062 + (pic_colorcolor) x -0.185

score=
  +Î²^0 
  +Î²^1Ã—ethnicity_not_minority 
  +Î²^3Ã—language_nonâˆ’english 
  +Î²^4+Ã—class_perc_eval 
  +Î²^5Ã—class_credits_one 
  +Î²^6Ã—bty_f1upper
  +Î²^7Ã—picture_color_colored

##16
###Verify that the conditions for this model are reasonable using diagnostic plots.
```{r}
p_hack_regression_points <- get_regression_points(rms.phack.model)

histo <- p_hack_regression_points %>% 
  ggplot() + 
  theme_light() +
  geom_histogram(aes(residual), fill = "lightgrey", color = "darkgrey") +
  labs(title = "Histogram of residuals")

qqplot <- p_hack_regression_points %>% 
  ggplot(aes(sample = residual)) +
  theme_light() +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "QQ of residuals")

absolute <- p_hack_regression_points %>% 
  ggplot() + 
  theme_light() +
  geom_point(aes(score_hat, abs(residual))) + 
  labs(title = "Absolute values of residuals against fitted values")

plot_grid(histo, qqplot, absolute)

eth <- evals %>% 
  ggplot() +
  geom_boxplot(aes(ethnicity, score))
  
gen <- evals %>% 
  ggplot() +
  geom_boxplot(aes(gender, score))
  
lan <- evals %>% 
  ggplot() +
  geom_boxplot(aes(language, score)) 

cre <- evals %>% 
  ggplot() +
  geom_boxplot(aes(language, score)) 

pic <- evals %>% 
  ggplot() +
  geom_boxplot(aes(pic_color, score)) 

bty <- evals %>% 
  ggplot() +
  geom_boxplot(aes(factor(bty_f1upper), score)) 

library(cowplot)
plot_grid(eth, gen, lan, cre, pic, bty, labels = c("A", "B", "C", "D", "E", "F"))

```

    * The residuals of the model is nearly normal as visible in the histogram and QQ plot; with a left skew.
    * Fitted vs absolute values are close. 
    * We assume independence based on study method.
    * boxplots show linearity related to categorical variables. 

##17
###The original paper describes how these data were gathered by taking a sample of professors from the University of Texas at Austin and including all courses that they have taught. Considering that each row represents a course, could this new information have an impact on any of the conditions of linear regression?

No. Even if the course is being taught by the same professor, Class courses are independent of each other so evaluation scores from one course is indpendent of the other

##18
###Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score.

A non minority male, who speaks English, teaches a one credit class, and has a high percent of students who evaluated.


##19
###Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not?

No, I most definitely would not as I can think of at least a few additional variables that might have an impact the scores of instructors outside predominantly white / male Austin, Texas. For example, per-capita of minorities in a university city might have an impact on the scores of minority professors. The per-capita _gender of students_ may play an impact of scores for the gender of professors. Just these two alone give me reason to pause.  The sampling of professors from Austin is not representative of the country as a whole; and most _certainly_ the students at a university in Austin are not representative of the country as a whole! 